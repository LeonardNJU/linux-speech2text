#!/usr/bin/env python3
"""
Whisperæ¨¡å‹æ€§èƒ½æµ‹è¯•è„šæœ¬
åŠŸèƒ½ï¼šæµ‹è¯•ä¸åŒWhisperæ¨¡å‹çš„è¯†åˆ«æ•ˆæœã€é€Ÿåº¦å’Œèµ„æºæ¶ˆè€—
"""

import os
import sys
import time
import json
import subprocess
import tempfile
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import psutil
import threading

@dataclass
class ModelResult:
    """æ¨¡å‹æµ‹è¯•ç»“æœ"""
    model_name: str
    text: str
    duration: float  # è¯†åˆ«è€—æ—¶(ç§’)
    file_size: int   # éŸ³é¢‘æ–‡ä»¶å¤§å°(å­—èŠ‚)
    cpu_usage: float # CPUä½¿ç”¨ç‡
    memory_usage: float # å†…å­˜ä½¿ç”¨é‡(MB)
    success: bool
    error: str = ""

class WhisperBenchmark:
    """Whisperæ¨¡å‹åŸºå‡†æµ‹è¯•å·¥å…·"""
    
    # Whisperå¯ç”¨æ¨¡å‹åˆ—è¡¨
    AVAILABLE_MODELS = [
        "tiny",     # æœ€å°æ¨¡å‹ï¼Œé€Ÿåº¦æœ€å¿«ï¼Œå‡†ç¡®ç‡è¾ƒä½
        "base",     # åŸºç¡€æ¨¡å‹ï¼Œå¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®ç‡
        "small",    # å°å‹æ¨¡å‹ï¼Œè¾ƒå¥½çš„å‡†ç¡®ç‡
        "medium",   # ä¸­å‹æ¨¡å‹ï¼Œæ›´å¥½çš„å‡†ç¡®ç‡
        "large",    # å¤§å‹æ¨¡å‹ï¼Œæœ€ä½³å‡†ç¡®ç‡ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢
        "large-v2", # å¤§å‹æ¨¡å‹v2ç‰ˆæœ¬
        "large-v3", # å¤§å‹æ¨¡å‹v3ç‰ˆæœ¬(æœ€æ–°)
    ]
    
    def __init__(self, test_audio_file: str = None, language: str = "Chinese"):
        self.test_audio_file = test_audio_file
        self.language = language
        self.results: List[ModelResult] = []
        self.temp_dir = tempfile.mkdtemp(prefix="whisper_test_")
        
    def check_dependencies(self) -> bool:
        """æ£€æŸ¥ä¾èµ–å·¥å…·"""
        try:
            result = subprocess.run(["whisper", "--help"], 
                                  capture_output=True, text=True)
            if result.returncode != 0:
                print("âŒ Whisperæœªå®‰è£…æˆ–ä¸å¯ç”¨")
                return False
            print("âœ… Whisperå¯ç”¨")
            return True
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°whisperå‘½ä»¤ï¼Œè¯·å…ˆå®‰è£… openai-whisper")
            return False
    
    def record_test_audio(self, duration: int = 10) -> str:
        """å½•åˆ¶æµ‹è¯•éŸ³é¢‘"""
        print(f"ğŸ“¹ å¼€å§‹å½•åˆ¶ {duration} ç§’æµ‹è¯•éŸ³é¢‘...")
        
        audio_file = os.path.join(self.temp_dir, "test_audio.wav")
        
        # ä½¿ç”¨ffmpegå½•åˆ¶éŸ³é¢‘
        cmd = [
            "ffmpeg", "-y", "-loglevel", "error",
            "-f", "pulse", "-i", "default",
            "-ac", "1", "-ar", "16000",
            "-t", str(duration),
            audio_file
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            print(f"âœ… å½•åˆ¶å®Œæˆ: {audio_file}")
            return audio_file
        except subprocess.CalledProcessError as e:
            print(f"âŒ å½•åˆ¶å¤±è´¥: {e}")
            return None
    
    def monitor_resources(self, pid: int, results: Dict) -> None:
        """ç›‘æ§è¿›ç¨‹èµ„æºä½¿ç”¨"""
        try:
            process = psutil.Process(pid)
            cpu_usage = []
            memory_usage = []
            
            while process.is_running():
                try:
                    cpu_usage.append(process.cpu_percent())
                    memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB
                    time.sleep(0.1)
                except psutil.NoSuchProcess:
                    break
            
            results['cpu_usage'] = sum(cpu_usage) / len(cpu_usage) if cpu_usage else 0
            results['memory_usage'] = max(memory_usage) if memory_usage else 0
            
        except Exception as e:
            print(f"âš ï¸ èµ„æºç›‘æ§å¤±è´¥: {e}")
            results['cpu_usage'] = 0
            results['memory_usage'] = 0
    
    def test_model(self, model_name: str, audio_file: str) -> ModelResult:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
        
        # è·å–éŸ³é¢‘æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(audio_file)
        
        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.join(self.temp_dir, f"{model_name}_output.txt")
        
        # æ„å»ºwhisperå‘½ä»¤
        cmd = [
            "whisper", audio_file,
            "--model", model_name,
            "--language", self.language,
            "--output_format", "txt",
            "--output_dir", self.temp_dir,
            "--verbose", "False"
        ]
        
        # ç›‘æ§èµ„æºä½¿ç”¨
        monitor_results = {}
        
        start_time = time.time()
        try:
            # å¯åŠ¨whisperè¿›ç¨‹
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, 
                                     stderr=subprocess.PIPE, text=True)
            
            # å¯åŠ¨èµ„æºç›‘æ§çº¿ç¨‹
            monitor_thread = threading.Thread(
                target=self.monitor_resources,
                args=(process.pid, monitor_results)
            )
            monitor_thread.start()
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            stdout, stderr = process.communicate()
            duration = time.time() - start_time
            
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹å®Œæˆ
            monitor_thread.join()
            
            if process.returncode == 0:
                # è¯»å–è¯†åˆ«ç»“æœ
                audio_basename = Path(audio_file).stem
                result_file = os.path.join(self.temp_dir, f"{audio_basename}.txt")
                
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        text = f.read().strip()
                except FileNotFoundError:
                    text = "æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
                
                return ModelResult(
                    model_name=model_name,
                    text=text,
                    duration=duration,
                    file_size=file_size,
                    cpu_usage=monitor_results.get('cpu_usage', 0),
                    memory_usage=monitor_results.get('memory_usage', 0),
                    success=True
                )
            else:
                return ModelResult(
                    model_name=model_name,
                    text="",
                    duration=duration,
                    file_size=file_size,
                    cpu_usage=monitor_results.get('cpu_usage', 0),
                    memory_usage=monitor_results.get('memory_usage', 0),
                    success=False,
                    error=stderr.strip()
                )
                
        except Exception as e:
            duration = time.time() - start_time
            return ModelResult(
                model_name=model_name,
                text="",
                duration=duration,
                file_size=file_size,
                cpu_usage=0,
                memory_usage=0,
                success=False,
                error=str(e)
            )
    
    def run_benchmark(self, models: List[str] = None, 
                     record_audio: bool = False, 
                     audio_duration: int = 10) -> List[ModelResult]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        
        if not self.check_dependencies():
            return []
        
        # ç¡®å®šè¦æµ‹è¯•çš„æ¨¡å‹
        if models is None:
            models = self.AVAILABLE_MODELS
        
        # ç¡®å®šæµ‹è¯•éŸ³é¢‘æ–‡ä»¶
        if record_audio or self.test_audio_file is None:
            self.test_audio_file = self.record_test_audio(audio_duration)
            if not self.test_audio_file:
                return []
        
        if not os.path.exists(self.test_audio_file):
            print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {self.test_audio_file}")
            return []
        
        print(f"ğŸµ ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶: {self.test_audio_file}")
        print(f"ğŸ“Š å¼€å§‹æµ‹è¯• {len(models)} ä¸ªæ¨¡å‹...")
        
        # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        self.results = []
        for i, model in enumerate(models, 1):
            print(f"\n[{i}/{len(models)}] " + "="*50)
            result = self.test_model(model, self.test_audio_file)
            self.results.append(result)
            
            if result.success:
                print(f"âœ… {model}: {result.duration:.2f}s, CPU: {result.cpu_usage:.1f}%, å†…å­˜: {result.memory_usage:.1f}MB")
                print(f"   è¯†åˆ«ç»“æœ: {result.text[:100]}...")
            else:
                print(f"âŒ {model}: å¤±è´¥ - {result.error}")
        
        return self.results
    
    def generate_report(self, output_file: str = "whisper_benchmark_report.json") -> None:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯ç”ŸæˆæŠ¥å‘Š")
            return
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in self.results if r.success]
        
        if not successful_results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
            return
        
        # æ‰¾å‡ºæœ€ä½³æ€§èƒ½æ¨¡å‹
        fastest_model = min(successful_results, key=lambda x: x.duration)
        lowest_cpu = min(successful_results, key=lambda x: x.cpu_usage)
        lowest_memory = min(successful_results, key=lambda x: x.memory_usage)
        
        # ç”ŸæˆæŠ¥å‘Šæ•°æ®
        report = {
            "test_info": {
                "audio_file": self.test_audio_file,
                "language": self.language,
                "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_models": len(self.results),
                "successful_models": len(successful_results)
            },
            "summary": {
                "fastest_model": fastest_model.model_name,
                "fastest_time": fastest_model.duration,
                "lowest_cpu_model": lowest_cpu.model_name,
                "lowest_cpu_usage": lowest_cpu.cpu_usage,
                "lowest_memory_model": lowest_memory.model_name,
                "lowest_memory_usage": lowest_memory.memory_usage
            },
            "detailed_results": [asdict(result) for result in self.results]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        # æ‰“å°ç®€è¦æŠ¥å‘Š
        self.print_summary()
    
    def print_summary(self) -> None:
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        if not self.results:
            return
        
        successful_results = [r for r in self.results if r.success]
        
        print("\n" + "="*60)
        print("ğŸ“Š Whisperæ¨¡å‹æ€§èƒ½æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        if not successful_results:
            print("âŒ æ‰€æœ‰æ¨¡å‹æµ‹è¯•éƒ½å¤±è´¥äº†")
            return
        
        # è¡¨æ ¼å¤´
        print(f"{'æ¨¡å‹':<12} {'è€—æ—¶(s)':<8} {'CPU(%)':<8} {'å†…å­˜(MB)':<10} {'çŠ¶æ€':<6}")
        print("-" * 60)
        
        # æ‰“å°æ¯ä¸ªæ¨¡å‹çš„ç»“æœ
        for result in self.results:
            status = "âœ…" if result.success else "âŒ"
            print(f"{result.model_name:<12} {result.duration:<8.2f} "
                  f"{result.cpu_usage:<8.1f} {result.memory_usage:<10.1f} {status:<6}")
        
        # æ¨è
        fastest = min(successful_results, key=lambda x: x.duration)
        lowest_resource = min(successful_results, key=lambda x: x.cpu_usage + x.memory_usage/100)
        
        print("\nğŸ† æ¨è:")
        print(f"   æœ€å¿«é€Ÿåº¦: {fastest.model_name} ({fastest.duration:.2f}s)")
        print(f"   æœ€ä½èµ„æº: {lowest_resource.model_name} (CPU: {lowest_resource.cpu_usage:.1f}%, å†…å­˜: {lowest_resource.memory_usage:.1f}MB)")
        
        # æ˜¾ç¤ºè¯†åˆ«æ–‡æœ¬å¯¹æ¯”
        print(f"\nğŸ“ è¯†åˆ«ç»“æœå¯¹æ¯”:")
        for result in successful_results:
            print(f"   {result.model_name}: {result.text[:80]}...")
    
    def cleanup(self) -> None:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

def main():
    parser = argparse.ArgumentParser(description="Whisperæ¨¡å‹æ€§èƒ½æµ‹è¯•å·¥å…·")
    parser.add_argument("--audio", "-a", help="æŒ‡å®šæµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
    parser.add_argument("--models", "-m", nargs="+", 
                       choices=WhisperBenchmark.AVAILABLE_MODELS,
                       help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹")
    parser.add_argument("--record", "-r", action="store_true",
                       help="å½•åˆ¶æ–°çš„æµ‹è¯•éŸ³é¢‘")
    parser.add_argument("--duration", "-d", type=int, default=10,
                       help="å½•åˆ¶éŸ³é¢‘æ—¶é•¿(ç§’)")
    parser.add_argument("--language", "-l", default="Chinese",
                       help="è¯†åˆ«è¯­è¨€")
    parser.add_argument("--output", "-o", default="whisper_benchmark_report.json",
                       help="æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    benchmark = WhisperBenchmark(
        test_audio_file=args.audio,
        language=args.language
    )
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = benchmark.run_benchmark(
            models=args.models,
            record_audio=args.record,
            audio_duration=args.duration
        )
        
        if results:
            # ç”ŸæˆæŠ¥å‘Š
            benchmark.generate_report(args.output)
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼Œæ— ç»“æœç”Ÿæˆ")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        benchmark.cleanup()

if __name__ == "__main__":
    main()